# ğŸ¤– IBM MQ Assistant Bot

<div align="center">

**An AI-Powered Conversational Interface for IBM MQ Management**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-FF4B4B.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

*Ask questions in natural language, get instant insights about your IBM MQ infrastructure*

</div>

---

## ğŸŒŸ Overview

IBM MQ Assistant Bot revolutionizes the way you interact with IBM MQ infrastructure. Instead of memorizing complex commands and navigating through multiple interfaces, simply **ask questions in plain English** and get instant, accurate responses powered by cutting-edge AI technology.

### âœ¨ Key Features

- ğŸ—£ï¸ **Natural Language Interface** - Ask questions like "How many queues are running on SRVIG?" instead of writing complex MQ commands
- ğŸ§  **Context-Aware Conversations** - Remembers your previous questions and queue manager context for seamless interactions
- âš¡ **Real-Time Insights** - Get immediate status updates, queue counts, channel information, and more
- ğŸ¯ **Smart Query Enhancement** - Automatically enriches vague questions with contextual information
- ğŸ”„ **Session Management** - Maintains conversation history with automatic session cleanup
- ğŸ¨ **Beautiful UI** - Clean, modern Streamlit interface with example questions and quick actions
- ğŸ”Œ **MCP Architecture** - Built on Model Context Protocol for seamless LLM-to-MQ communication

---

## ğŸ—ï¸ Architecture

This bot is part of a **multi-tier AI-powered MQ management system**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MQ Assistant Bot      â”‚  â† You are here! (Streamlit UI)
â”‚   (This Repo)           â”‚     â€¢ User interface
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ LLM integration
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLM             â”‚  â† AI Analysis
â”‚   (Llama 3.1/Ollama)    â”‚     â€¢ Analyzes questions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Determines MCP tool calls
            â”‚ MCP Protocol
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MQ-MCP Repo         â”‚  â† MCP Server
â”‚   (MCP Tools)           â”‚     â€¢ Implements MCP tools
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Calls FastAPI
            â”‚ HTTP
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   fastapi-app Repo      â”‚  â† REST API Wrapper
â”‚     (FastAPI)           â”‚     â€¢ Wraps IBM MQ REST API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Handles authentication
            â”‚ HTTP/REST
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IBM MQ REST API       â”‚  â† IBM MQ REST Interface
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Queue Manager Infra    â”‚  â† Actual IBM MQ
â”‚   (SRVIG, etc.)         â”‚     â€¢ Queue Managers
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Queues & Channels
```

### ğŸ”— Dependencies

This bot requires two additional repositories to be running:

1. **[MQ-MCP](https://github.com/vignesh1988i/MQ-MCP)** - MCP Server for IBM MQ
   - Implements MCP tools (list_queues, get_qmgr_status, etc.)
   - Called by LLM via MCP protocol
   - Makes HTTP calls to fastapi-app for MQ operations
   - Must be running and properly configured

2. **[fastapi-app](https://github.com/vignesh1988i/fastapi-app)** - REST API Wrapper
   - Wraps IBM MQ REST API with simplified endpoints
   - Called by MQ-MCP tools via HTTP
   - Handles IBM MQ REST API authentication and requests
   - Must be running and configured with IBM MQ REST API credentials

---

## âš¡ Startup Order

**IMPORTANT:** Components must be started in this specific order for the bot to function properly:

```
1ï¸âƒ£  IBM Queue Manager (QMGR)
    â””â”€ Your IBM MQ infrastructure must be running
    â””â”€ Queue managers (e.g., SRVIG) must be active
    â””â”€ Verify: dspmq command shows RUNNING status

2ï¸âƒ£  MQWEB Server
    â””â”€ IBM MQ REST API server must be started
    â””â”€ Provides REST endpoints for MQ operations
    â””â”€ Verify: Access https://localhost:9443/ibmmq/console

3ï¸âƒ£  FastAPI Application (fastapi-app)
    â””â”€ Start the FastAPI wrapper service
    â””â”€ Connects to IBM MQ REST API
    â””â”€ Verify: curl http://localhost:<port>/health

4ï¸âƒ£  MCP Ollama Bridge (MQ-MCP)
    â””â”€ Start the MCP server
    â””â”€ Connects to FastAPI for MQ data
    â””â”€ Verify: Check MCP server logs for "running"

5ï¸âƒ£  MQ Assistant Bot (This Application)
    â””â”€ Finally, start this Streamlit application
    â””â”€ Run: streamlit run app.py
    â””â”€ Access: http://localhost:8501
```

**âš ï¸ If components are not started in order, you may experience connection errors or tool execution failures.**

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Running instance of **MQ-MCP** server
- Running instance of **fastapi-app** (mcp-ollama-bridge)
- IBM MQ environment (for MQ-MCP to connect to)
wraps IBM MQ REST API)
- IBM MQ environment with REST API enabled

1. **Clone the repository**
   ```bash
   git clone https://github.com/vignesh1988i/mq-assistant-bot.git
   cd mq-assistant-bot
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   
   Create a `.env` file in the root directory:
   ```env
   MCP_BRIDGE_URL=http://localhost:8090/api/chat
   LLM_MODEL=llama3.1:8b
   SESSION_TIMEOUT_MINUTES=30
   MAX_CONVERSATION_HISTORY=10
   ```

5. **Start the bot**
   ```bash
   streamlit run app.py
   ```

6. **Open your browser**
   
   Navigate to `http://localhost:8501` and start asking questions!

---

## ğŸ’¬ Usage Examples

Once the application is running, try these questions:

| Question Type | Example |
|--------------|---------|
| Queue Manager Status | "Is SRVIG running?" |
| Queue Count | "How many queues are in SRVIG?" |
| Queue Search | "Show me all SYSTEM queues in SRVIG" |
| Queue Details | "Tell me about DEV.QUEUE.1 in SRVIG" |
| Channel Information | "How many channels does SRVIG have?" |
| List Queue Managers | "List all queue managers" |

### ğŸ§© Context-Aware Conversations

The bot remembers your context:

```
You: Is SRVIG running?
Bot: Yes, SRVIG is running with 45 queues and 12 channels.

You: How many queues does it have?  ğŸ‘ˆ "it" refers to SRVIG
Bot: SRVIG has 45 queues.

You: Show me the SYSTEM queues  ğŸ‘ˆ Automatically queries SRVIG
Bot: Here are the SYSTEM queues in SRVIG: ...
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_BRIDGE_URL` | `http://localhost:8090/api/chat` | URL endpoint for LLM communication |
| `LLM_MODEL` | `llama3.1:8b` | LLM model to use via Ollama |
| `SESSION_TIMEOUT_MINUTES` | `30` | Session inactivity timeout |
| `MAX_CONVERSATION_HISTORY` | `10` | Maximum messages to keep in history |

---

## ğŸ”„ How It Works

### The Complete Flow

1. **You ask a question** â†’ "How many queues are in SRVIG?"
2. **Streamlit UI captures** â†’ Sends to embedded LLM
3. **LLM analyzes** â†’ Llama 3.1 understands the intent
4. **LLM determines MCP tools** â†’ Needs to call `list_queues` with qmgr=SRVIG
5. **MCP tool executes** â†’ MQ-MCP receives the tool call via MCP protocol
6. **MQ-MCP calls FastAPI** â†’ HTTP request to fastapi-app endpoints
7. **FastAPI calls IBM MQ REST API** â†’ Gets actual queue data from MQ
8. **Data flows back** â†’ IBM MQ â†’ fastapi-app â†’ MQ-MCP â†’ LLM formats response
9. **Bot displays result** â†’ "SRVIG has 45 queues: DEV.QUEUE.1, ..."

### Session Management

- Each conversation gets a unique session ID
- Sessions expire after 30 minutes of inactivity
- Conversation history is maintained for context
- Queue manager context is preserved across questions

---

## ğŸ“ Project Structure

```
mq-assistant-bot/
â”œâ”€â”€ app.py              # Streamlit UI application
â”œâ”€â”€ agent.py            # Agent logic and session management
â”œâ”€â”€ config.py           # Configuration and environment variables
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env               # Environment configuration (create this)
â”œâ”€â”€ .gitignore         # Git ignore rules
â””â”€â”€ README.md          # This file
```

---

## ğŸ› Troubleshooting

### Bot shows "Connection Error"
- Ensure **fastapi-app** is running on the configured URL
- Check `LLM (Ollama with Llama 3.1) is running and accessible
- Check if MQ-MCP server is running
- Verify MCP protocol connectivity

### MCP tool execution fails
- Ensure **fastapi-app** is running
- Check fastapi-app logs for HTTP errors
- Verify fastapi-appREST API credentials and connectivity

### LLM responses are slow
- LLM inference happens locally via Ollama
- Consider using a smaller model (e.g., `llama3.1:7b`)
- Check system resources (CPU/GPU/Memory)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Vignesh SR**

*AI Engineer passionate about making enterprise systems more accessible through conversational AI*

---

## ğŸ™ Acknowledgments

- Built on the **Model Context Protocol (MCP)** specification
- Powered by **Ollama** and **Llama 3.1**
- UI framework by **Streamlit**
- IBM MQ integration via custom MCP server

---

<div align="center">

**â­ Star this repo if you find it useful!**

Made with â¤ï¸ for the IBM MQ community

</div>
