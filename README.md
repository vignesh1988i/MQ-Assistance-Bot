# ğŸ¤– IBM MQ Assistant Bot

<div align="center">

**An AI-Powered Conversational Interface for IBM MQ Management**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-FF4B4B.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

*Ask questions in natural language, get instant insights about your IBM MQ infrastructure*

</div>

---

## ğŸŒŸ Overview

IBM MQ Assistant Bot revolutionizes the way you interact with IBM MQ infrastructure. Instead of memorizing complex commands and navigating through multiple interfaces, simply **ask questions in plain English** and get instant, accurate responses powered by cutting-edge AI technology.

### âœ¨ Key Features

- ğŸ—£ï¸ **Natural Language Interface** - Ask questions like "How many queues are running on SRVIG?" instead of writing complex MQ commands
- ğŸ§  **Context-Aware Conversations** - Remembers your previous questions and queue manager context for seamless interactions
- âš¡ **Real-Time Insights** - Get immediate status updates, queue counts, channel information, and more
- ğŸ¯ **Smart Query Enhancement** - Automatically enriches vague questions with contextual information
- ğŸ”„ **Session Management** - Maintains conversation history with automatic session cleanup
- ğŸ¨ **Beautiful UI** - Clean, modern Streamlit interface with example questions and quick actions
- ğŸ”Œ **MCP Architecture** - Built on Model Context Protocol for seamless LLM-to-MQ communication

---

## ğŸ—ï¸ Architecture

This bot is part of a **three-tier AI-powered MQ management system**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MQ Assistant Bot      â”‚  â† You are here! (Frontend + Orchestration)
â”‚   (Streamlit UI)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ HTTP
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  mcp-ollama-bridge      â”‚  â† LLM Interaction Layer
â”‚  (FastAPI)              â”‚     â€¢ Processes natural language
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Determines tool calls
            â”‚ MCP                â€¢ Formats responses
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MQ-MCP             â”‚  â† MQ Integration Layer
â”‚  (MCP Server)           â”‚     â€¢ Executes MQ commands
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Returns structured data
```

### ğŸ”— Dependencies

This bot requires two additional repositories to be running:

1. **[MQ-MCP](https://github.com/yourusername/MQ-MCP)** - MCP server that interfaces directly with IBM MQ
   - Executes actual MQ commands (MQSC, PCF)
   - Returns structured queue, channel, and qmgr data
   - Must be running and accessible

2. **[fastapi-app](https://github.com/yourusername/fastapi-app)** (mcp-ollama-bridge) - The AI brain
   - Hosts the LLM (Llama 3.1 via Ollama)
   - Translates natural language to MCP tool calls
   - Formats LLM responses for the UI
   - Must be running on the configured bridge URL

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Running instance of **MQ-MCP** server
- Running instance of **fastapi-app** (mcp-ollama-bridge)
- IBM MQ environment (for MQ-MCP to connect to)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/mq-assistant-bot.git
   cd mq-assistant-bot
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   
   Create a `.env` file in the root directory:
   ```env
   MCP_BRIDGE_URL=http://localhost:8090/api/chat
   LLM_MODEL=llama3.1:8b
   SESSION_TIMEOUT_MINUTES=30
   MAX_CONVERSATION_HISTORY=10
   ```

5. **Start the bot**
   ```bash
   streamlit run app.py
   ```

6. **Open your browser**
   
   Navigate to `http://localhost:8501` and start asking questions!

---

## ğŸ’¬ Usage Examples

Once the application is running, try these questions:

| Question Type | Example |
|--------------|---------|
| Queue Manager Status | "Is SRVIG running?" |
| Queue Count | "How many queues are in SRVIG?" |
| Queue Search | "Show me all SYSTEM queues in SRVIG" |
| Queue Details | "Tell me about DEV.QUEUE.1 in SRVIG" |
| Channel Information | "How many channels does SRVIG have?" |
| List Queue Managers | "List all queue managers" |

### ğŸ§© Context-Aware Conversations

The bot remembers your context:

```
You: Is SRVIG running?
Bot: Yes, SRVIG is running with 45 queues and 12 channels.

You: How many queues does it have?  ğŸ‘ˆ "it" refers to SRVIG
Bot: SRVIG has 45 queues.

You: Show me the SYSTEM queues  ğŸ‘ˆ Automatically queries SRVIG
Bot: Here are the SYSTEM queues in SRVIG: ...
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_BRIDGE_URL` | `http://localhost:8090/api/chat` | URL of the mcp-ollama-bridge (FastAPI app) |
| `LLM_MODEL` | `llama3.1:8b` | LLM model to use (configured in bridge) |
| `SESSION_TIMEOUT_MINUTES` | `30` | Session inactivity timeout |
| `MAX_CONVERSATION_HISTORY` | `10` | Maximum messages to keep in history |

---

## ğŸ”„ How It Works

### The mcp-ollama-bridge Connection

The **mcp-ollama-bridge** is where the AI magic happens:

1. **You ask a question** â†’ "How many queues are in SRVIG?"
2. **Bot sends to bridge** â†’ HTTP request to `/api/chat` endpoint
3. **Bridge consults LLM** â†’ Llama 3.1 understands the intent
4. **LLM determines tools** â†’ Needs to call `list_queues` with qmgr=SRVIG
5. **Bridge calls MQ-MCP** â†’ Executes the MCP tool via protocol
6. **MQ-MCP queries IBM MQ** â†’ Gets actual queue data
7. **Bridge formats response** â†’ LLM creates natural language answer
8. **Bot displays result** â†’ "SRVIG has 45 queues: DEV.QUEUE.1, ..."

### Session Management

- Each conversation gets a unique session ID
- Sessions expire after 30 minutes of inactivity
- Conversation history is maintained for context
- Queue manager context is preserved across questions

---

## ğŸ“ Project Structure

```
mq-assistant-bot/
â”œâ”€â”€ app.py              # Streamlit UI application
â”œâ”€â”€ agent.py            # Agent logic and session management
â”œâ”€â”€ config.py           # Configuration and environment variables
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env               # Environment configuration (create this)
â”œâ”€â”€ .gitignore         # Git ignore rules
â””â”€â”€ README.md          # This file
```

---

## ğŸ› Troubleshooting

### Bot shows "Connection Error"
- Ensure **fastapi-app** (mcp-ollama-bridge) is running on the configured URL
- Check `MCP_BRIDGE_URL` in your `.env` file
- Verify bridge is accessible: `curl http://localhost:8090/health`

### Bridge returns "Tool execution failed"
- Ensure **MQ-MCP** server is running and accessible
- Check MQ-MCP server logs for connection errors
- Verify IBM MQ environment is accessible to MQ-MCP

### LLM responses are slow
- LLM inference happens on the bridge side
- Consider using a smaller model (e.g., `llama3.1:7b`)
- Check bridge server resources (CPU/GPU)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Vignesh SR**

*AI Engineer passionate about making enterprise systems more accessible through conversational AI*

---

## ğŸ™ Acknowledgments

- Built on the **Model Context Protocol (MCP)** specification
- Powered by **Ollama** and **Llama 3.1**
- UI framework by **Streamlit**
- IBM MQ integration via custom MCP server

---

<div align="center">

**â­ Star this repo if you find it useful!**

Made with â¤ï¸ for the IBM MQ community

</div>
